{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to build a image classifier that can classify the images<br>\n",
    "in the test dataset to some classes of the images.<br>\n",
    "We will train our model from training dataset given in the problem.<br>\n",
    "Then fit the model into the testing data set and store it into a CSV file<br>\n",
    "with the following details:<br>\n",
    "    Test_image_index, predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data given is in .pkl format we have to extract the data first so we import the Pickel library.<br><br>\n",
    "<b>Intro to pickle format:</b><br>\n",
    "Pickling is used to store python objects. This means things like lists, dictionaries, class objects, and more.<br>\n",
    "while saving the data in PKL format it's nature is also saved.For ex. if we save a dictionary to a pickle format file<br>\n",
    "then when loading it into a variable it's nature will be dictionary no need to worry about that.<br>\n",
    "In simple it is saving the pyhton objects in stream of bytes which can then be accessed backed easily.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to deserialise the training data into a python object.And then use it to our testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motto behind the pickling is suppose if we are having a very large dataset in any format say mysql or .csv file<br>\n",
    "When extracting that data to use it in our program will take a lot of time instead pickling will not.<br>\n",
    "You can refer further to pickling to the following link:<br>\n",
    "    <a href=\"https://pythonprogramming.net/python-pickle-module-save-objects-serialization/\">Know more about Pickling</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we all know about some basic library\n",
    "that is <b>numpy</b> or numerical python library which consisting of multidimensional array<br>\n",
    "objects and a collection of routines for processing those arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv #for storing the predicted result into the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we will use TensorFlow library which is specially used for branch of machine learning<br>\n",
    "that is Deep Learning.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to know more about tensorflow here is the link:<br>\n",
    "    <a href=\"https://cv-tricks.com/artificial-intelligence/deep-learning/deep-learning-frameworks/tensorflow/tensorflow-tutorial/\">Know more Tensorflow</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the train dataset from the train_label.pkl file given in the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_label.pkl','rb') as f:\n",
    "    #As our file is stored in streams of bytes.We open it as read in byte mode.\n",
    "    train_label=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract the train Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_image.pkl','rb') as f:\n",
    "    train_image=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put the train image to the numpy array.So that we can use tensorflow to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=np.array(train_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise it more better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A method in matplot library to know the dataset structure.\n",
    "train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look to the above results.<br>\n",
    "The train label lenght is 8000 meaning there are 8000 images.<br>\n",
    "784 is length of the train image which is basically a grayscale 28*28 pixels image.<br>\n",
    "What we are going to do is to store this image into a 3d array of 8000*28*28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=train_image.reshape(8000,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentions behind doing this:\n",
    "<br>\n",
    "We all know about the neural network.Which takes some inputs,process it according<br>\n",
    "to a certain algorithm and gives the output.We are passing a image into the inputs which<br>\n",
    "are the nodes of total no 28*28=784.Training our model according to it and then predicting<br>\n",
    "the resulting output.<br>\n",
    "If you want to know more about this then go to the following link.<br>\n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">Neural networks</a><br>\n",
    "<img src=\"2.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to reshpae our test data as of the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test_image .pkl','rb') as f:\n",
    "    test_image=pickle.load(f)\n",
    "test_image=np.array(train_image)\n",
    "test_image=test_image.reshape(8000,28,28)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now we were shaping our data.<br>\n",
    "Algorithm our network is using:<br>\n",
    "    <ol>\n",
    "    <li>We are passing the image as pixels of 28*28 i.e  784 into the inputs of neural network<br>\n",
    "    in form of bits either 0 or 1.So we have divide each pixel of our image by 255.</li>\n",
    "    <li>As explained below then we going to get which class a particular image belongs to</li>\n",
    "    <li>Basically we are first training our model with the training dataset by passing the images<br>\n",
    "    of training dataset and then as given classes.</li>\n",
    "    <li>Next fit out model to test data and save it in a .csv(comma seperated value file)</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 2s 209us/sample - loss: 12.0986 - acc: 0.2494\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 1s 122us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 1s 121us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 1s 123us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s 147us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s 119us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s 119us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 1s 121us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 1s 134us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 1s 140us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 1s 138us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 1s 123us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 1s 122us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 1s 120us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 1s 119us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 1s 123us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s 117us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 1s 128us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 1s 124us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 1s 118us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 1s 120us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s 119us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 1s 119us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 1s 121us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s 120us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 1s 133us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s 137us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 1s 128us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s 134us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s 135us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s 137us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s 146us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s 131us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 1s 125us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 1s 152us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 1s 132us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 1s 120us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 1s 121us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 1s 120us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 1s 118us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 1s 121us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 121us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 123us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 165us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 142us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 132us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 137us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 120us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 100us/sample - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 100us/sample - loss: 12.0886 - acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2758646d940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "#Passing our data\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#fitting our model\n",
    "model.fit(train_image, train_label, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as our model is trained by passing the train dataset into the model.We now predicts our test data<br>\n",
    "and create a 2d matrix to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting our test data\n",
    "predict_test_data=model.predict(test_image)\n",
    "image_index=list(range(0,8000))\n",
    "#index of image is a list\n",
    "predicted_class=[]\n",
    "#for storing the predicted values.\n",
    "for i in range (8000):\n",
    "    predicted_class.append(np.argmax(predict_test_data[i]))\n",
    "#Storing it to a CSV file\n",
    "d_matrix=list(zip(image_index,predicted_class))\n",
    "#inserting the values\n",
    "d_matrix.insert(0,('Test_image_index','predicted_class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now storing to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RamdevGodara.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(d_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Conclusion:</h3><br>\n",
    "    Just build a image classifier using a very simple neural network whose methods are even defined in Tensorflow library.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
